language: python

###############################################################################
# Cache data which has to be downloaded on every build.
# This is just for Travis and doesn't do anything on Shippable.
cache:
  directories:
    # Cache files downloaded by pip
    - $HOME/.cache/pip
    # Cache our miniconda download.
    # Cautionary note: if a new version of Python is released and added to
    # conda and you want to test on the new version, your cached copy will be
    # out of date and you'll need to wipe the cache. Details are here
    # https://docs.travis-ci.com/user/caching/#Clearing-Caches
    # but if this sounds like too much hassle, you can just comment out this
    # line to stop caching.
    - $HOME/Downloads

###############################################################################
python:
  # This is a flag for the built-in version of Python provided by the CI-server
  # provider, which we don't use in favour of conda. But we use this to pick
  # out which python version we install with conda, since it means the provider
  # gets appropriate metadata to keep things organised.
  # - "2.6" # Not well supported by unittest - no skip decorators available.
  - "2.7"
  - "3.3"
  - "3.4"
  - "3.5"

###############################################################################
env:
  # Run tests with newest and oldest dependencies
  - USE_OLDEST_DEPENDENCIES="false"
  - USE_OLDEST_DEPENDENCIES="true"

###############################################################################
# Setup the environment before installing
before_install:
  # Remember the directory where our repository to test is located
  - pwd
  - REPOPATH="$(pwd)"
  # ---------------------------------------------------------------------------
  # Check which versions of numpy and scipy we are using
  - if [ -f requirements.txt ]; then
      NUMPY_REQUIREMENT="$(grep '^numpy\([<>=!]\|$\)' requirements.txt)";
      SCIPY_REQUIREMENT="$(grep '^scipy\([<>=!]\|$\)' requirements.txt)";
    fi;
  # ---------------------------------------------------------------------------
  # If we want to run the tests using the oldest set of dependencies we
  # support, modify any *requirements*.txt files every '>=' becomes '=='.
  - if [[ "$USE_OLDEST_DEPENDENCIES" == "true" ]]; then
      for FILE in *requirements*.txt; do
          sed -e 's/>=/==/g' $FILE > $FILE.tmp && mv $FILE.tmp $FILE;
      done;
    fi;
  # ---------------------------------------------------------------------------
  # Update the package list
  - sudo apt-get update
  # Install scipy dependencies with apt-get. We only need ATLAS.
  - if [[ "$SCIPY_REQUIREMENT" != "" ]]; then
      sudo apt-get install -y libatlas3gf-base libatlas-dev;
    fi;
  # ---------------------------------------------------------------------------
  # The following is based on Minicoda's how-to Travis page
  # http://conda.pydata.org/docs/travis.html
  # ---------------------------------------------------------------------------
  # Download miniconda. Only do this if the cached file isn't present.
  - mkdir -p $HOME/Downloads
  - if [ ! -f $HOME/Downloads/miniconda.sh ]; then
      wget https://repo.continuum.io/miniconda/Miniconda-latest-Linux-x86_64.sh -O "$HOME/Downloads/miniconda.sh";
    fi;
  # Install miniconda to the home directory.
  # May have to remove an old copy from the last build if we're on Shippable
  - if [ -d $HOME/miniconda ]; then
      rm -r $HOME/miniconda;
    fi;
  - bash $HOME/Downloads/miniconda.sh -b -p $HOME/miniconda;
  - export PATH="$HOME/miniconda/bin:$PATH"
  - hash -r
  # Automatically say yes to any check from conda
  - conda config --set always_yes yes --set changeps1 no
  - conda update -q conda
  # Useful for debugging any issues with conda
  - conda info -a
  # Create the conda environment with pip, numpy and scipy installed (if they
  # are in requirements.txt)
  - conda create -q -n test-environment python=$TRAVIS_PYTHON_VERSION
      pip $NUMPY_REQUIREMENT $SCIPY_REQUIREMENT
  # Activate the test environment
  - source activate test-environment
  # ---------------------------------------------------------------------------
  # Define a helper function which installs dependencies from a list in a file,
  # which uses conda where it can and pip when it can't.
  #
  # First, we remove the version requirement and get just the package name.
  # Then we search the conda database to see if a package with this exact name
  # is present. We already updated our cache, so we don't need to ask the
  # server again, and just use the cached copy. The output of this is a
  # header line and then a list of matching package names. We then grep
  # this to check whether one of the lines of the output is an exact match
  # for the name of the package we want to install.
  - function conda_or_pip_install_packages {
        while read PV; do
          echo "";
          echo "==================================================================";
          PN="$(echo $PV | sed 's/^\([^=<>]*\).*/\1/')";
          if echo "$PV" | grep -qE '^#';
          then
            continue;
          elif echo "$PV" | grep -qEv '^-e|.*\+.*|.*git://' &&
            conda search $PN --full-name --use-index-cache --names-only |
              grep -qFxi $PN;
          then
            USEPIP=0;
            echo "Package $PN is on conda. Installing it from there.";
            echo "------------------------------------------------------------------";
            conda install -q "$PV";
            ERRORCODE=$?;
          else
            USEPIP=1;
          fi;
          if [[ $ERRORCODE -ne 0 ]] || [[ $USEPIP -gt 0 ]];
          then
            echo "Package $PV isn't on conda. Trying to install it from PyPI.";
            echo "------------------------------------------------------------------";
            pip install -U "$PV";
            ERRORCODE=$?;
          fi;
          if [[ $ERRORCODE -ne 0 ]]; then return 1; fi;
        done < $1;
    }

###############################################################################
install:
  # Install required packages listed in requirements.txt. Use conda if they are
  # on there, and pip if they aren't. It is much faster to install from conda
  # because the packages are pre-compiled, but not everything is there.
  - if [ -f requirements.txt ]; then
      cat requirements.txt;
      conda_or_pip_install_packages requirements.txt;
    fi;
  # Also install any developmental requirements, if present.
  - if [ -f requirements-dev.txt ]; then
      cat requirements-dev.txt;
      conda_or_pip_install_packages requirements-dev.txt;
    fi;
  - if [ -f requirements-test.txt ]; then
      cat requirements-test.txt;
      conda_or_pip_install_packages requirements-test.txt;
    fi;
  # ---------------------------------------------------------------------------
  # Now install your own package, e.g.
  # - python setup.py install

###############################################################################
before_script:
  # Double-check we are still in the right directory
  - pwd
  # Check what python packages we have installed
  - conda info -a
  - pip freeze
  # ---------------------------------------------------------------------------
  # Set up folders for test results
  - if [ "$SHIPPABLE" = "true" ]; then
      mkdir -p shippable/testresults;
      mkdir -p shippable/codecoverage;
    fi;

###############################################################################
script:
  - which python
  - pip freeze
  # ---------------------------------------------------------------------------
  # Your test script goes here, e.g.
  # - python setup.py test
  - py.test --flake8 --junitxml=testresults.xml --cov=package_name --cov-report xml --cov-config .coveragerc

###############################################################################
after_script:
  # Show where we ended up
  - pwd
  # Go back to the repository directory, just in case
  - cd ${REPOPATH}
  # Show what results files there are
  - ls -alh
  # ---------------------------------------------------------------------------
  # Move results and coverage files into appropriate places
  - if [ "$SHIPPABLE" = "true" ] && [ -f testresults.xml ]; then
      mv testresults.xml shippable/testresults/;
    fi;
  - if [ "$SHIPPABLE" = "true" ] && [ -f coverage.xml ]; then
      cp coverage.xml shippable/codecoverage/;
    fi;
  # ---------------------------------------------------------------------------
  # Make an environment.yml artifact file. Since this build was successful,
  # you can share it for users to install from with the command
  # `conda env create -f environment.yml` and know they have a working build.
  # If you want to save this file, you will need to turn on the artifacts addon
  # or do something else with it. See here for details about artifacts
  # https://docs.travis-ci.com/user/uploading-artifacts/
  - conda env export > environment.yml && cat environment.yml

###############################################################################
after_success:
  # Only run coveralls on Travis. When running on a public Travis-CI, the
  # repo token is automatically inferred, but to run coveralls on Shippable
  # the repo token needs to be specified in a .coveralls.yml or as an
  # environment variable COVERALLS_REPO_TOKEN. This should be kept hidden
  # from public viewing, either by encrypting the token or running on a
  # private build.
  - if [ "$TRAVIS" = "true" ] && [ "$SHIPPABLE" != "true" ]; then
      coveralls;
    fi;

###############################################################################
# Steps to take before archiving on Shippable (does nothing on Travis)
before_archive:
  # Have shippable archive the environment.yml artifact by putting it in
  # the REPO/shippable folder. This is available to download as a tar file for
  # each build.
  - if [ "$SHIPPABLE" = "true" ] && [ -f environment.yml ]; then
      cp environment.yml shippable/;
    fi;

###############################################################################
# Enable archiving of artifacts on Shippable (does nothing on Travis)
archive: true
